{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central problem for a neural network implementation is this: during the forward pass, you compute results that will later be useful during the backward pass. How do you keep track of this arbitrary state, while making sure that layers can be cleanly composed?\n",
    "\n",
    "Most libraries solve this problem by having you declare the forward computations, which are then compiled into a graph somewhere behind the scenes. Thinc doesn't have a \"computational graph\". Instead, we just use the stack, because we put the state from the forward pass into callbacks.\n",
    "\n",
    "https://github.com/explosion/thinc\n",
    "\n",
    "All nodes in the network have a simple signature:\n",
    "\n",
    "``` python\n",
    "f(inputs) -> {outputs, f(d_outputs)->d_inputs}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(filename='debug.log',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(inputs):\n",
    "    mask = inputs > 0\n",
    "    def backprop_relu(d_outputs, optimizer, **kwargs):\n",
    "        return d_outputs * mask\n",
    "    return inputs * mask, backprop_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_layer(n_out, n_in):\n",
    "    W = np.random.randn(n_out, n_in)\n",
    "    b = np.zeros((n_out, 1))\n",
    "\n",
    "    def forward(X):\n",
    "        Y = W @ X + b\n",
    "        logging.debug(\"[->]: %s, %s, %s\" % (Y.shape, W.shape, b.shape))\n",
    "        def backward_linear(dY, optimizer, **kwargs):\n",
    "            logging.debug(\"[<-]: %d, %d\" % dY.shape)\n",
    "            dX = W.T @ dY\n",
    "            dW = np.einsum('ik,jk->ij', dY, X)\n",
    "            db = dY.sum(axis=1, keepdims=True)\n",
    "           \n",
    "            optimizer(W, dW)\n",
    "            optimizer(b, db)\n",
    "\n",
    "            return dX\n",
    "        return Y, backward_linear\n",
    "    return forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chain(*layers):\n",
    "    '''\n",
    "    Chain together layers\n",
    "    '''\n",
    "    def forward(X):\n",
    "        backprops = []\n",
    "        Y = X\n",
    "        for layer in layers:\n",
    "            Y, backprop = layer(Y)\n",
    "            backprops.append(backprop)\n",
    "        \n",
    "        def backward(dY, optimizer, **kwargs):\n",
    "            for backprop in reversed(backprops):\n",
    "                dY = backprop(dY, optimizer, **kwargs)\n",
    "            return dY\n",
    "        \n",
    "        return Y, backward\n",
    "    \n",
    "    return forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(theta, dtheta, alpha = 0.001):\n",
    "    theta -= alpha * dtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mse_layer():\n",
    "    def forward(X):\n",
    "        cost = lambda y: np.average((X - y)**2)\n",
    "        \n",
    "        def backward_mse(dY, optimizer, y):\n",
    "            dX = -2 * (y - X)\n",
    "            return dX\n",
    "        \n",
    "        return cost, backward_mse\n",
    "    \n",
    "    return forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, model, optimizer, epochs = 10):    \n",
    "    for i in range(epochs):\n",
    "        cost, backpropagate = model(X)\n",
    "        backpropagate(1, sgd, y = y)\n",
    "\n",
    "        yield i, cost(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data(N, features_num, m=3):\n",
    "    X = np.random.randn(features_num, N)\n",
    "    W1 = np.random.randn(m, features_num)\n",
    "    W2 = np.random.randn(1, m)\n",
    "\n",
    "    y = W1 @ X\n",
    "    y = W2 @ (y *(y > 0))\n",
    "    \n",
    "    return  X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "NUM_EXAMPLES = 100\n",
    "NUM_FEATURES = 5\n",
    "\n",
    "X, y = data(NUM_EXAMPLES, NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "affine_1 = create_linear_layer(3, 5)\n",
    "affine_2 = create_linear_layer(1, 3)\n",
    "mse = create_mse_layer()\n",
    "\n",
    "mlp = chain(affine_1, relu, affine_2, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [(i, c) for (i, c) in train(\n",
    "    X, y,\n",
    "    epochs = EPOCHS, \n",
    "    model = mlp,\n",
    "    optimizer = sgd\n",
    ")] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs, ys = zip(*costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Cost per epoch.\")\n",
    "plt.plot(xs, ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will show, how can we implement operator overloading with context manager to be able to specify our model in a following manner:\n",
    "\n",
    "```python\n",
    "with Model.define_operators({\">>\": chain_models}):\n",
    "    model = (Input(100, 5)\n",
    "        >> Affine(3, 5)\n",
    "        >> ReLu() \n",
    "        >> Affine(1, 3) \n",
    "        >> MSE()\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    _operators = {}\n",
    "    \n",
    "    def __rshift__(self, other):\n",
    "        '''Apply the function bound to the '>>' operator.'''\n",
    "        return self._operators['>>'](self, other)\n",
    "    \n",
    "    @classmethod\n",
    "    @contextlib.contextmanager\n",
    "    def define_operators(cls, operators):\n",
    "        old_ops = dict(cls._operators)\n",
    "        for op, func in operators.items():\n",
    "            cls._operators[op] = func\n",
    "        yield\n",
    "        cls._operators = old_ops\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine(Model):\n",
    "    def __init__(self, n_out, n_in):\n",
    "        self.forward = create_linear_layer(n_out, n_in) \n",
    "\n",
    "class ReLu(Model):\n",
    "    def __init__(self):\n",
    "        self.forward = relu\n",
    "    \n",
    "class MSE(Model):\n",
    "    def __init__(self):\n",
    "        self.forward = create_mse_layer()\n",
    "        \n",
    "class FeedForward(Model):\n",
    "    def __init__(self, layers):\n",
    "        self._layers = []\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, FeedForward):\n",
    "                self._layers.extend(layer._layers)\n",
    "            else:\n",
    "                self._layers.append(layer)\n",
    "    \n",
    "    def __call__(self):\n",
    "        '''\n",
    "        Chain together layers\n",
    "        '''\n",
    "        def forward(X):\n",
    "            backprops = []\n",
    "            Y = X\n",
    "            for layer in self._layers:\n",
    "                Y, backprop = layer(Y)\n",
    "                backprops.append(backprop)\n",
    "\n",
    "            def backward(dY, optimizer, **kwargs):\n",
    "                for backprop in reversed(backprops):\n",
    "                    dY = backprop(dY, optimizer, **kwargs)\n",
    "                return dY\n",
    "\n",
    "            return Y, backward\n",
    "    \n",
    "        return forward\n",
    "\n",
    "class Input(Model):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.forward = Input.create_identity(n_in, n_out)\n",
    "        \n",
    "    @classmethod  \n",
    "    def create_identity(self, n_in, n_out):\n",
    "        def forward(X):\n",
    "            def backward_id(dY, optimizer, **kwargs):\n",
    "                logging.debug(\"[<-]: %d, %d\" % dY.shape)\n",
    "                return np.ones((n_in, n_out))\n",
    "\n",
    "            return X, backward_id\n",
    "    \n",
    "        return forward\n",
    "\n",
    "def chain_models(*layers):\n",
    "    if len(layers) == 0:\n",
    "        return FeedForward([])\n",
    "    elif len(layers) == 1:\n",
    "        return layers[0]\n",
    "    else:\n",
    "        return FeedForward(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Model.define_operators({\">>\": chain_models}):\n",
    "    model = (Input(100, 5) | Input(100, 5)\n",
    "        >> Affine(3, 5)\n",
    "        >> ReLu() \n",
    "        >> Affine(1, 3) \n",
    "        >> MSE()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [(i, c) for (i, c) in train(\n",
    "    X, y,\n",
    "    epochs = EPOCHS, \n",
    "    model = model(),\n",
    "    optimizer = sgd\n",
    ")] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = zip(*costs)\n",
    "plt.title(\"Cost per epoch.\")\n",
    "plt.plot(xs, ys);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
